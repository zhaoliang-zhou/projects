{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from scipy.stats import uniform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_predict, cross_val_score, GroupKFold, RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel, RFECV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n",
    "from sklearn.metrics import make_scorer, mean_poisson_deviance, mean_gamma_deviance\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector, TransformedTargetRegressor\n",
    "from sklearn.preprocessing import PowerTransformer, OneHotEncoder, StandardScaler, PolynomialFeatures, OrdinalEncoder\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, PoissonRegressor, GammaRegressor, TweedieRegressor, LogisticRegressionCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingRegressor, StackingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import LinearSVR, LinearSVC, SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data\n",
    "df = pd.read_csv('InsNova_train.csv')\n",
    "df = df.sample(frac=1.0)\n",
    "df.loc[:, 'pure_premium'] = df['claim_cost'] / df['exposure']\n",
    "df.loc[:, 'severity'] = df['claim_cost'] / np.fmax(df['claim_count'], 1)\n",
    "df.loc[:, 'frequency'] = df['claim_count'] / df['exposure']\n",
    "\n",
    "# Getting CV inds\n",
    "n_folds = 10\n",
    "cv = StratifiedKFold(n_folds, shuffle=True, random_state=123)\n",
    "df.loc[:, 'fold'] = 0\n",
    "for fold, (_, test_inds) in enumerate(cv.split(df, df['claim_ind'])):\n",
    "    df.loc[test_inds, 'fold'] = fold\n",
    "    \n",
    "# Feature engineering\n",
    "df['large_veh'] = np.where(df['veh_body'].isin(['MIBUS', 'MCARA', 'BUS']), 1, 0)\n",
    "df['expensive_area'] = np.where(df['area'].isin(['E','F']), 1, 0)\n",
    "df['expensive_age_risk'] = np.where(df['dr_age'].isin([1, 2]) & (df['veh_value'] > 5.0), 1, 0)\n",
    "df['expensive_veh'] = np.where(df['veh_value'] > 6.0, 1, 0)\n",
    "df['severe_veh'] = np.where(df['veh_body'].isin(['HDTOP', 'TRUCK', 'UTE']), 1, 0)\n",
    "df['young'] = np.where(df['dr_age'] == 1, 1, 0)\n",
    "\n",
    "# Creating Categorical dataset for LightGBM and CatBoost\n",
    "df['lm_gender'] = np.where(df['gender'] == 'M', 1, 0)\n",
    "for i in ['veh_body', 'area', 'gender', 'large_veh', 'expensive_area', 'expensive_age_risk', 'expensive_veh', 'severe_veh', 'young']:\n",
    "    df[i] = df[i].astype('category')\n",
    "df['dr_age'] = df['dr_age'].astype(np.float64)\n",
    "df['veh_age'] = df['veh_age'].astype(np.float64) \n",
    "\n",
    "# Splitting into pred/response\n",
    "response_cols = ['fold',\n",
    "                 'exposure',\n",
    "                 'claim_ind',\n",
    "                 'claim_count',\n",
    "                 'claim_cost',\n",
    "                 'pure_premium',\n",
    "                 'severity',\n",
    "                 'frequency']\n",
    "X, y = df.drop(response_cols, axis=1), df[response_cols]\n",
    "X = X.drop('id', axis=1)\n",
    "X['exposure'] = y['exposure'].copy()\n",
    "lin_cols = ['veh_value', 'veh_body', 'veh_age', 'lm_gender', 'area', 'dr_age']\n",
    "boost_cols = ['veh_value', 'veh_body', 'veh_age', 'gender', 'area', 'dr_age']\n",
    "lin_sev_cols = lin_cols + ['exposure']\n",
    "boost_sev_cols = boost_cols + ['exposure']\n",
    "\n",
    "# Importing test set\n",
    "df_test = pd.read_csv('InsNova_test.csv')\n",
    "df_test['lm_gender'] = np.where(df_test['gender'] == 'M', 1, 0)\n",
    "df_test['large_veh'] = np.where(df_test['veh_body'].isin(['MIBUS', 'MCARA', 'BUS']), 1, 0)\n",
    "df_test['expensive_area'] = np.where(df_test['area'].isin(['E','F']), 1, 0)\n",
    "df_test['expensive_age_risk'] = np.where(df_test['dr_age'].isin([1, 2]) & (df_test['veh_value'] > 5.0), 1, 0)\n",
    "df_test['expensive_veh'] = np.where(df_test['veh_value'] > 6.0, 1, 0)\n",
    "df_test['severe_veh'] = np.where(df_test['veh_body'].isin(['HDTOP', 'TRUCK', 'UTE']), 1, 0)\n",
    "df_test['young'] = np.where(df_test['dr_age'] == 1, 1, 0)\n",
    "\n",
    "for i in ['veh_body', 'area', 'gender', 'large_veh', 'expensive_area', 'expensive_age_risk', 'expensive_veh', 'severe_veh', 'young']:\n",
    "    df_test[i] = df_test[i].astype('category')\n",
    "    \n",
    "df_test['dr_age'] = df_test['dr_age'].astype(np.float64)\n",
    "df_test['veh_age'] = df_test['veh_age'].astype(np.float64) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our gini function\n",
    "def gini(y_true, y_pred):\n",
    "    # check and get number of samples\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    n_samples = y_true.shape[0]\n",
    "    \n",
    "    # sort rows on prediction column \n",
    "    # (from largest to smallest)\n",
    "    arr = np.array([y_true, y_pred]).transpose()\n",
    "    true_order = arr[arr[:,0].argsort()][::-1,0]\n",
    "    pred_order = arr[arr[:,1].argsort()][::-1,0]\n",
    "    \n",
    "    # get Lorenz curves\n",
    "    L_true = np.cumsum(true_order) / np.sum(true_order)\n",
    "    L_pred = np.cumsum(pred_order) / np.sum(pred_order)\n",
    "    L_ones = np.linspace(1/n_samples, 1, n_samples)\n",
    "    \n",
    "    # get Gini coefficients (area between curves)\n",
    "    G_true = np.sum(L_ones - L_true)\n",
    "    G_pred = np.sum(L_ones - L_pred)\n",
    "    \n",
    "    # normalize to true Gini coefficient\n",
    "    return G_pred / G_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining column transformers for later steps          \n",
    "get_cats = make_column_selector(dtype_include=pd.CategoricalDtype)\n",
    "get_notcats = make_column_selector(dtype_exclude=pd.CategoricalDtype)\n",
    "get_notfloats = make_column_selector(dtype_exclude=np.float64)\n",
    "get_floats = make_column_selector(dtype_include=np.float64)\n",
    "get_ints = make_column_selector(dtype_include=[np.int32, np.int64])\n",
    "one_hot = lambda: ColumnTransformer([('one_hot', OneHotEncoder(drop='first', sparse=False), get_cats)], remainder='passthrough')\n",
    "\n",
    "# Initializing cross validated preds\n",
    "cv_freq_preds = {}\n",
    "test_freq_preds = {}\n",
    "cv_sev_preds = {}\n",
    "test_sev_preds = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a target encoder\n",
    "class TargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, use_median=True, nonzero=True, min_samples=10):\n",
    "        self.nonzero = nonzero\n",
    "        self.use_median = use_median\n",
    "        self.min_samples = min_samples\n",
    "        self.categories_ = None\n",
    "        self.medians_ = None\n",
    "        self.all_mu = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        _X, _y = np.array(X), np.array(y)\n",
    "        self.categories_ = {i: np.unique(_X[:, i]) for i in range(_X.shape[1])}\n",
    "        self.mu_ = {k:{i:[] for i in v} for k, v in self.categories_.items()}\n",
    "        mu_func = np.median if self.use_median else np.mean\n",
    "        self.all_mu = mu_func(_y[_y > 0.0]) if self.nonzero else mu_func(_y)\n",
    "        for k, v in self.categories_.items():\n",
    "            for i in v:\n",
    "                if self.nonzero:\n",
    "                    _x = _y[(_y > 0.0) & (_X[:, k] == i)]\n",
    "                    if _x.shape[0] == 0:\n",
    "                        self.mu_[k][i] = 0.0\n",
    "                    elif _x.shape[0] < self.min_samples:\n",
    "                        self.mu_[k][i] = self.all_mu\n",
    "                    else:\n",
    "                        self.mu_[k][i] = mu_func(_x)\n",
    "                else:\n",
    "                    _x = _y[_X[:, k] == i]\n",
    "                    if _x.shape[0] < self.min_samples:\n",
    "                        self.mu_[k][i] = self.all_mu\n",
    "                    else:\n",
    "                        self.mu_[k][i] = mu_func(_x)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        _X = np.array(X)\n",
    "        for i in range(_X.shape[1]):\n",
    "            _X[:, i] = [self.mu_[i][j] if j in self.mu_.keys() else self.all_mu for j in _X[:, i]]\n",
    "        return np.array(_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32581236185788\n",
      "1.3437953419221085\n"
     ]
    }
   ],
   "source": [
    "# Freq sev predictions\n",
    "freq_lm = make_pipeline(ColumnTransformer([('target_enc', TargetEncoder(True, True, 20), get_cats),\n",
    "                                          ('box-cox', PowerTransformer(standardize=False), get_notcats)], remainder='passthrough'),\n",
    "                             PolynomialFeatures(degree=2, interaction_only=True, include_bias=False),\n",
    "                             StandardScaler(),\n",
    "                             PoissonRegressor(alpha=1.0, max_iter=1000))\n",
    "\n",
    "# Getting predictions\n",
    "ginis, poissons, test_preds, train_preds = [], [], [], []\n",
    "for i in range(n_folds):\n",
    "    X_train, X_test, y_train, y_test = X.loc[y['fold'] != i, :], X.loc[y['fold'] == i, :], y.loc[y['fold'] != i, :], y.loc[y['fold'] == i, :]\n",
    "    freq_lm.fit(X_train[lin_cols], y_train['frequency'], poissonregressor__sample_weight=y_train['exposure'])\n",
    "    train_preds.append(pd.Series(freq_lm.predict(X_test[lin_cols]), index=X_test.index))\n",
    "    test_preds.append(freq_lm.predict(df_test[lin_cols]))\n",
    "    poissons.append(mean_poisson_deviance(y_test['frequency'], train_preds[-1]))\n",
    "    ginis.append(gini(y_test['claim_count'], train_preds[-1] * y_test['exposure']))\n",
    "print(np.mean(ginis))\n",
    "print(np.mean(poissons))\n",
    "test_freq_preds['freq_lm'] = np.mean(test_preds, axis=0)\n",
    "cv_freq_preds['freq_lm'] = pd.concat(train_preds).loc[y.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32734163673833805\n",
      "1.3447274983494935\n"
     ]
    }
   ],
   "source": [
    "# LGBM freq\n",
    "csts = {'veh_value': -1, 'dr_age': -1, 'veh_age': 1}\n",
    "freq_lgbm = LGBMRegressor(n_estimators=250,\n",
    "                          learning_rate=0.01,\n",
    "                          subsample=0.8,\n",
    "                          subsample_freq=1,\n",
    "                          objective='poisson',\n",
    "                          monotone_constraints=[csts[i] if i in csts.keys() else 0 for i in boost_cols],\n",
    "                          monotone_constraints_method='advanced',\n",
    "                          num_leaves=16,\n",
    "                          n_jobs=-1)\n",
    "\n",
    "# Getting predictions\n",
    "ginis, poissons, test_preds, train_preds = [], [], [], []\n",
    "for i in range(n_folds):\n",
    "    X_train, X_test, y_train, y_test = X.loc[y['fold'] != i, :], X.loc[y['fold'] == i, :], y.loc[y['fold'] != i, :], y.loc[y['fold'] == i, :]\n",
    "    freq_lgbm.fit(X_train[boost_cols], y_train['frequency'], sample_weight=y_train['exposure'])\n",
    "    train_preds.append(pd.Series(freq_lgbm.predict(X_test[boost_cols]), index=X_test.index))\n",
    "    test_preds.append(freq_lgbm.predict(df_test[boost_cols]))\n",
    "    poissons.append(mean_poisson_deviance(y_test['frequency'], train_preds[-1]))\n",
    "    ginis.append(gini(y_test['claim_count'], train_preds[-1] * y_test['exposure']))\n",
    "print(np.mean(ginis))\n",
    "print(np.mean(poissons))\n",
    "test_freq_preds['freq_lgbm'] = np.mean(test_preds, axis=0)\n",
    "cv_freq_preds['freq_lgbm'] = pd.concat(train_preds).loc[y.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3266319491597976\n",
      "1.337405816344505\n"
     ]
    }
   ],
   "source": [
    "# Catboost freq\n",
    "csts = {'veh_value': -1, 'dr_age': -1, 'veh_age': 1}\n",
    "freq_cat = CatBoostRegressor(n_estimators=250,\n",
    "                             learning_rate=0.05,\n",
    "                             objective='Poisson',\n",
    "                             max_depth=3,\n",
    "                             cat_features=[i for i in boost_cols if i not in ['veh_value', 'dr_age', 'veh_age']],\n",
    "                             monotone_constraints=[csts[i] if i in csts.keys() else 0 for i in boost_cols],\n",
    "                             verbose=0,\n",
    "                             thread_count=-1)\n",
    "\n",
    "# Getting predictions\n",
    "ginis, poissons, test_preds, train_preds = [], [], [], []\n",
    "for i in range(n_folds):\n",
    "    X_train, X_test, y_train, y_test = X.loc[y['fold'] != i, :], X.loc[y['fold'] == i, :], y.loc[y['fold'] != i, :], y.loc[y['fold'] == i, :]\n",
    "    freq_cat.fit(X_train[boost_cols], y_train['frequency'], sample_weight=y_train['exposure'])\n",
    "    train_preds.append(pd.Series(freq_cat.predict(X_test[boost_cols]), index=X_test.index))\n",
    "    test_preds.append(freq_cat.predict(df_test[boost_cols]))\n",
    "    poissons.append(mean_poisson_deviance(y_test['frequency'], train_preds[-1]))\n",
    "    ginis.append(gini(y_test['claim_count'], train_preds[-1] * y_test['exposure']))\n",
    "print(np.mean(ginis))\n",
    "print(np.mean(poissons))\n",
    "test_freq_preds['freq_cat'] = np.mean(test_preds, axis=0)\n",
    "cv_freq_preds['freq_cat'] = pd.concat(train_preds).loc[y.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3270711767754437\n",
      "1.3437223555033369\n"
     ]
    }
   ],
   "source": [
    "# Freq sev predictions\n",
    "csts = {'veh_value': -1, 'dr_age': -1, 'veh_age': 1}\n",
    "freq_hgb = make_pipeline(ColumnTransformer([('target_enc', OrdinalEncoder(), get_cats)], remainder='passthrough'),\n",
    "                        HistGradientBoostingRegressor(loss='poisson',\n",
    "                                                      max_iter=150,\n",
    "                                                      learning_rate=0.01,\n",
    "                                                      max_leaf_nodes=16,\n",
    "                                                      l2_regularization=10.0,\n",
    "                                                      monotonic_cst=[csts[i] if i in csts.keys() else 0 for i in lin_cols]\n",
    "                                                     ))\n",
    "\n",
    "# Getting predictions\n",
    "ginis, poissons, test_preds, train_preds = [], [], [], []\n",
    "for i in range(n_folds):\n",
    "    X_train, X_test, y_train, y_test = X.loc[y['fold'] != i, :], X.loc[y['fold'] == i, :], y.loc[y['fold'] != i, :], y.loc[y['fold'] == i, :]\n",
    "    freq_hgb.fit(X_train[lin_cols], y_train['frequency'], histgradientboostingregressor__sample_weight=y_train['exposure'])\n",
    "    train_preds.append(pd.Series(freq_hgb.predict(X_test[lin_cols]), index=X_test.index))\n",
    "    test_preds.append(freq_hgb.predict(df_test[lin_cols]))\n",
    "    poissons.append(mean_poisson_deviance(y_test['frequency'], train_preds[-1]))\n",
    "    ginis.append(gini(y_test['claim_count'], train_preds[-1] * y_test['exposure']))\n",
    "print(np.mean(ginis))\n",
    "print(np.mean(poissons))\n",
    "test_freq_preds['freq_hgb'] = np.mean(test_preds, axis=0)\n",
    "cv_freq_preds['freq_hgb'] = pd.concat(train_preds).loc[y.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3316748343767709\n",
      "1.3315047301414107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('target_enc',\n",
       "                                                  TargetEncoder(min_samples=20),\n",
       "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x00000190B1DF1FD0>),\n",
       "                                                 ('box-cox',\n",
       "                                                  PowerTransformer(standardize=False),\n",
       "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x00000190B1DF1820>)])),\n",
       "                ('polynomialfeatures',\n",
       "                 PolynomialFeatures(include_bias=False, interaction_only=True)),\n",
       "                ('standardscaler', StandardScaler()),\n",
       "                ('poissonregressor',\n",
       "                 PoissonRegressor(alpha=0.0, max_iter=1000))])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stacking frequency\n",
    "\"\"\"\n",
    "csts = {'veh_value': 1, 'dr_age': -1}#, **{i: 1 for i in list(cv_freq_preds.keys())}}\n",
    "X_cv = pd.concat([X[boost_cols], pd.DataFrame(cv_freq_preds)], axis=1)\n",
    "freq_stack = CatBoostRegressor(n_estimators=250,\n",
    "                              objective='Poisson',\n",
    "                              max_depth=3,\n",
    "                              cat_features=[i for i in X_cv.columns if i in ['veh_body', 'gender', 'area']],\n",
    "                              monotone_constraints=[csts[i] if i in csts.keys() else 0 for i in X_cv.columns],\n",
    "                              verbose=0,\n",
    "                              thread_count=-1)\n",
    "\"\"\"\n",
    "X_cv = pd.concat([X[lin_cols], pd.DataFrame(cv_freq_preds)], axis=1)\n",
    "freq_stack = make_pipeline(ColumnTransformer([('target_enc', TargetEncoder(True, True, 20), get_cats),\n",
    "                                          ('box-cox', PowerTransformer(standardize=False), get_notcats)], remainder='passthrough'),\n",
    "                             PolynomialFeatures(degree=2, interaction_only=True, include_bias=False),\n",
    "                             StandardScaler(),\n",
    "                             PoissonRegressor(alpha=0.0, max_iter=1000))\n",
    "ginis, poissons, test_preds, train_preds = [], [], [], []\n",
    "for i in range(n_folds):\n",
    "    X_train, X_test, y_train, y_test = X_cv.loc[y['fold'] != i, :], X_cv.loc[y['fold'] == i, :], y.loc[y['fold'] != i, :], y.loc[y['fold'] == i, :]\n",
    "    freq_stack.fit(X_train, y_train['frequency'], poissonregressor__sample_weight=y_train['exposure'])\n",
    "    train_preds.append(pd.Series(freq_stack.predict(X_test), index=X_test.index))\n",
    "    poissons.append(mean_poisson_deviance(y_test['frequency'], train_preds[-1]))\n",
    "    ginis.append(gini(y_test['claim_count'], train_preds[-1] * y_test['exposure']))\n",
    "freq_stack_preds = pd.concat(train_preds).loc[y.index]\n",
    "print(np.mean(ginis))\n",
    "print(np.mean(poissons))\n",
    "freq_stack.fit(X_cv, y['frequency'], poissonregressor__sample_weight=y['exposure'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3529.0204771797626, 2815.024979999179, 4081.8189032353675, 5172.526737799155, 3272.3520169106987, 4631.933348214872, 2657.1611718854406, 3354.4975732842254, 3898.7425292050725, 2725.341713578022]\n",
      "3613.84194512918\n"
     ]
    }
   ],
   "source": [
    "# Freq sev predictions\n",
    "sev_lm = make_pipeline(ColumnTransformer([('target_enc', TargetEncoder(True, True, 20), get_cats),\n",
    "                                       ('box-cox', PowerTransformer(standardize=False), get_notcats)], remainder='passthrough'),\n",
    "                           PolynomialFeatures(degree=2, interaction_only=True, include_bias=False),\n",
    "                           StandardScaler(),\n",
    "                           GammaRegressor(alpha=5.0, max_iter=1000))\n",
    "\n",
    "# Getting predictions\n",
    "sevs, test_preds, train_preds = [], [], []\n",
    "for i in range(n_folds):\n",
    "    X_train, X_test, y_train, y_test = X.loc[y['fold'] != i, :], X.loc[y['fold'] == i, :], y.loc[y['fold'] != i, :], y.loc[y['fold'] == i, :]\n",
    "    sev_mask = y_train['claim_cost'] > 0.0\n",
    "    sev_lm.fit(X_train.loc[sev_mask, lin_sev_cols], y_train.loc[sev_mask, 'severity'], gammaregressor__sample_weight=y_train.loc[sev_mask, 'claim_count'])\n",
    "    preds = sev_lm.predict(X_test[lin_sev_cols])\n",
    "    train_preds.append(pd.Series(preds, index=X_test.index))\n",
    "    test_preds.append(sev_lm.predict(df_test[lin_sev_cols]))\n",
    "    sevs.append(np.sqrt(np.mean((y_test.loc[y_test['claim_cost'] > 0.0, 'severity'] - preds[y_test['claim_cost'] > 0.0]) ** 2)))\n",
    "print(sevs)\n",
    "print(np.mean(sevs))\n",
    "test_sev_preds['sev_lm'] = np.mean(test_preds, axis=0)\n",
    "cv_sev_preds['sev_lm'] = pd.concat(train_preds).loc[y.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3540.615215490788, 2849.6489045031053, 4113.466521714538, 5209.456981145867, 3298.49516049426, 4663.789875459549, 2639.691821437647, 3420.88360107951, 3929.000196972192, 2728.6923337466865]\n",
      "3639.3740612044144\n"
     ]
    }
   ],
   "source": [
    "# Freq sev predictions\n",
    "csts = {'veh_value': 1, 'dr_age': -1}\n",
    "sev_lgbm = LGBMRegressor(n_estimators=250,\n",
    "                         learning_rate=0.001,\n",
    "                         subsample=0.67,\n",
    "                         subsample_freq=1,\n",
    "                         colsample_bytree=0.7,\n",
    "                         objective='gamma',\n",
    "                         monotone_constraints=[csts[i] if i in csts.keys() else 0 for i in boost_sev_cols],\n",
    "                         monotone_constraints_method='advanced',\n",
    "                         num_leaves=8,\n",
    "                         n_jobs=-1)\n",
    "\n",
    "# Getting predictions\n",
    "sevs, test_preds, train_preds = [], [], []\n",
    "for i in range(n_folds):\n",
    "    X_train, X_test, y_train, y_test = X.loc[y['fold'] != i, :], X.loc[y['fold'] == i, :], y.loc[y['fold'] != i, :], y.loc[y['fold'] == i, :]\n",
    "    sev_mask = y_train['claim_cost'] > 0.0\n",
    "    sev_lgbm.fit(X_train.loc[sev_mask, boost_sev_cols], y_train.loc[sev_mask, 'severity'], sample_weight=y_train.loc[sev_mask, 'claim_count'])\n",
    "    preds = sev_lgbm.predict(X_test[boost_sev_cols])\n",
    "    train_preds.append(pd.Series(preds, index=X_test.index))\n",
    "    test_preds.append(sev_lgbm.predict(df_test[boost_sev_cols]))\n",
    "    sevs.append(np.sqrt(np.mean((y_test.loc[y_test['claim_cost'] > 0.0, 'severity'] - preds[y_test['claim_cost'] > 0.0]) ** 2)))\n",
    "print(sevs)\n",
    "print(np.mean(sevs))\n",
    "test_sev_preds['sev_lgbm'] = np.mean(test_preds, axis=0)\n",
    "cv_sev_preds['sev_lgbm'] = pd.concat(train_preds).loc[y.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3497.7289459117055, 2856.3648559883663, 4043.1903636399293, 5115.251104929736, 3354.740513348025, 4616.050566931161, 2768.2199511293747, 3312.530069127422, 3909.892992766295, 2797.398845694033]\n",
      "3627.1368209466045\n"
     ]
    }
   ],
   "source": [
    "# Freq sev predictions\n",
    "csts = {'veh_value': 1, 'dr_age': -1}\n",
    "sev_cat = CatBoostRegressor(n_estimators=250,\n",
    "                            subsample=0.67,\n",
    "                            max_depth=1,\n",
    "                            cat_features=[i for i in boost_sev_cols if i not in ['veh_value', 'dr_age', 'veh_age', 'exposure']],\n",
    "                            monotone_constraints=[csts[i] if i in csts.keys() else 0 for i in boost_sev_cols],\n",
    "                            verbose=0,\n",
    "                            thread_count=-1)\n",
    "\n",
    "# Getting predictions\n",
    "sevs, test_preds, train_preds = [], [], []\n",
    "for i in range(n_folds):\n",
    "    X_train, X_test, y_train, y_test = X.loc[y['fold'] != i, :], X.loc[y['fold'] == i, :], y.loc[y['fold'] != i, :], y.loc[y['fold'] == i, :]\n",
    "    sev_mask = y_train['claim_cost'] > 0.0\n",
    "    sev_cat.fit(X_train.loc[sev_mask, boost_sev_cols], y_train.loc[sev_mask, 'severity'], sample_weight=y_train.loc[sev_mask, 'claim_count'])\n",
    "    preds = sev_cat.predict(X_test[boost_sev_cols])\n",
    "    train_preds.append(pd.Series(preds, index=X_test.index))\n",
    "    test_preds.append(sev_cat.predict(df_test[boost_sev_cols]))\n",
    "    sevs.append(np.sqrt(np.mean((y_test.loc[y_test['claim_cost'] > 0.0, 'severity'] - preds[y_test['claim_cost'] > 0.0]) ** 2)))\n",
    "print(sevs)\n",
    "print(np.mean(sevs))\n",
    "test_sev_preds['sev_cat'] = np.mean(test_preds, axis=0)\n",
    "cv_sev_preds['sev_cat'] = pd.concat(train_preds).loc[y.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3529.452078683232, 2847.838486977578, 4072.6913499285947, 5153.532316631541, 3249.1653213926515, 4646.163938449369, 2698.039733432999, 3311.8160483775937, 3887.1469339187784, 2751.8140731014305]\n",
      "3614.766028089377\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "knn_sev = make_pipeline(ColumnTransformer([('target_enc', TargetEncoder(True, True, 20), get_cats)\n",
    "                                      ], remainder='passthrough'),\n",
    "                    StandardScaler(),\n",
    "                    BaggingRegressor(KNeighborsRegressor(n_neighbors=25, n_jobs=-1),\n",
    "                                     n_estimators=25,\n",
    "                                     max_features=5,\n",
    "                                     max_samples=0.8))\n",
    "# Scoring model\n",
    "sevs, test_preds, train_preds = [], [], []\n",
    "for i in range(n_folds):\n",
    "    X_train, X_test, y_train, y_test = X.loc[y['fold'] != i, :], X.loc[y['fold'] == i, :], y.loc[y['fold'] != i, :], y.loc[y['fold'] == i, :]\n",
    "    sev_mask = y_train['claim_cost'] > 0.0\n",
    "    knn_sev.fit(X_train.loc[sev_mask, lin_sev_cols], y_train.loc[sev_mask, 'severity'])\n",
    "    preds = knn_sev.predict(X_test[lin_sev_cols])\n",
    "    train_preds.append(pd.Series(preds, index=X_test.index))\n",
    "    test_preds.append(knn_sev.predict(df_test[lin_sev_cols]))\n",
    "    sevs.append(np.sqrt(np.mean((y_test.loc[y_test['claim_cost'] > 0.0, 'severity'] - preds[y_test['claim_cost'] > 0.0]) ** 2)))\n",
    "print(sevs)\n",
    "print(np.mean(sevs))\n",
    "test_sev_preds['knn_sev'] = np.mean(test_preds, axis=0)\n",
    "cv_sev_preds['knn_sev'] = pd.concat(train_preds).loc[y.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3586.617710066896, 2894.0394224510237, 4071.463089854397, 5049.901637906729, 3301.8593826943365, 4568.192992506632, 2819.4527305897864, 3305.9229654739447, 3888.9738726335413, 2828.2990422175703]\n",
      "3631.4722846394857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "mlp_sev = make_pipeline(ColumnTransformer([('target_enc', TargetEncoder(True, True, 20), get_cats)\n",
    "                                           ], remainder='passthrough'),\n",
    "                    StandardScaler(),\n",
    "                    TransformedTargetRegressor(MLPRegressor(64, max_iter=15), transformer=StandardScaler()))\n",
    "# Scoring model\n",
    "sevs, test_preds, train_preds = [], [], []\n",
    "for i in range(n_folds):\n",
    "    X_train, X_test, y_train, y_test = X.loc[y['fold'] != i, :], X.loc[y['fold'] == i, :], y.loc[y['fold'] != i, :], y.loc[y['fold'] == i, :]\n",
    "    sev_mask = y_train['claim_cost'] > 0.0\n",
    "    mlp_sev.fit(X_train.loc[sev_mask, lin_sev_cols], y_train.loc[sev_mask, 'severity'])\n",
    "    preds = mlp_sev.predict(X_test[lin_sev_cols])\n",
    "    train_preds.append(pd.Series(preds, index=X_test.index))\n",
    "    test_preds.append(mlp_sev.predict(df_test[lin_sev_cols]))\n",
    "    sevs.append(np.sqrt(np.mean((y_test.loc[y_test['claim_cost'] > 0.0, 'severity'] - preds[y_test['claim_cost'] > 0.0]) ** 2)))\n",
    "print(sevs)\n",
    "print(np.mean(sevs))\n",
    "test_sev_preds['mlp_sev'] = np.mean(test_preds, axis=0)\n",
    "cv_sev_preds['mlp_sev'] = pd.concat(train_preds).loc[y.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3565.312306651559, 2835.7191415571842, 4012.126617772698, 5100.035685000249, 3257.9810275375844, 4602.567479627455, 2773.649647787202, 3282.855983943877, 3883.4892718258798, 2772.556702425881]\n",
      "3608.629386412957\n"
     ]
    }
   ],
   "source": [
    "# Linear SVR\n",
    "lsvr_sev = make_pipeline(ColumnTransformer([('target_enc', TargetEncoder(True, True, 20), get_cats)\n",
    "                                      ], remainder='passthrough'),\n",
    "                    StandardScaler(),\n",
    "                    TransformedTargetRegressor(LinearSVR(loss='squared_epsilon_insensitive', C=1.0, max_iter=3000), transformer=StandardScaler()))\n",
    "# Scoring model\n",
    "sevs, test_preds, train_preds = [], [], []\n",
    "for i in range(n_folds):\n",
    "    X_train, X_test, y_train, y_test = X.loc[y['fold'] != i, :], X.loc[y['fold'] == i, :], y.loc[y['fold'] != i, :], y.loc[y['fold'] == i, :]\n",
    "    sev_mask = y_train['claim_cost'] > 0.0\n",
    "    lsvr_sev.fit(X_train.loc[sev_mask, lin_sev_cols], y_train.loc[sev_mask, 'severity'], transformedtargetregressor__sample_weight=y_train.loc[sev_mask, 'claim_count'])\n",
    "    preds = lsvr_sev.predict(X_test[lin_sev_cols])\n",
    "    train_preds.append(pd.Series(preds, index=X_test.index))\n",
    "    test_preds.append(lsvr_sev.predict(df_test[lin_sev_cols]))\n",
    "    sevs.append(np.sqrt(np.mean((y_test.loc[y_test['claim_cost'] > 0.0, 'severity'] - preds[y_test['claim_cost'] > 0.0]) ** 2)))\n",
    "print(sevs)\n",
    "print(np.mean(sevs))\n",
    "test_sev_preds['lsvr_sev'] = np.mean(test_preds, axis=0)\n",
    "cv_sev_preds['lsvr_sev'] = pd.concat(train_preds).loc[y.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3445.6631591031373, 2800.196533799697, 3972.789483369281, 4837.240751045394, 3238.3553134302038, 4544.9229512757165, 2576.4942228369905, 3217.2341980084434, 3869.4024244406605, 2735.197072048798]\n",
      "3523.7496109358317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('target_enc',\n",
       "                                                  TargetEncoder(min_samples=20),\n",
       "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x00000190B1DF1FD0>)])),\n",
       "                ('standardscaler', StandardScaler()),\n",
       "                ('transformedtargetregressor',\n",
       "                 TransformedTargetRegressor(regressor=LinearSVR(C=3.0,\n",
       "                                                                loss='squared_epsilon_insensitive',\n",
       "                                                                max_iter=10000),\n",
       "                                            transformer=StandardScaler()))])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stacking all severities\n",
    "csts = {'veh_value': 1, 'dr_age': -1}#, **{i: 1 for i in list(cv_sev_preds.keys())}}\n",
    "X_cv = pd.concat([X[lin_sev_cols], pd.DataFrame(cv_sev_preds)], axis=1)\n",
    "\"\"\"\n",
    "sev_stack = LGBMRegressor(n_estimators=50,\n",
    "                         learning_rate=0.01,\n",
    "                         #subsample=0.67,\n",
    "                         #subsample_freq=1,\n",
    "                         #colsample_bytree=0.7,\n",
    "                         #objective='gamma',\n",
    "                         #monotone_constraints=[csts[i] if i in csts.keys() else 0 for i in X_cv.columns],\n",
    "                         #monotone_constraints_method='advanced',\n",
    "                         num_leaves=6,\n",
    "                         n_jobs=-1)\n",
    "                         \"\"\"\n",
    "sev_stack = make_pipeline(ColumnTransformer([('target_enc', TargetEncoder(True, True, 20), get_cats)], remainder='passthrough'),\n",
    "                    StandardScaler(),\n",
    "                    TransformedTargetRegressor(LinearSVR(loss='squared_epsilon_insensitive', C=3.0, max_iter=10000), transformer=StandardScaler()))\n",
    "\n",
    "# Scoring model\n",
    "sevs, test_preds, train_preds = [], [], []\n",
    "for i in range(n_folds):\n",
    "    X_train, X_test, y_train, y_test = X_cv.loc[y['fold'] != i, :], X_cv.loc[y['fold'] == i, :], y.loc[y['fold'] != i, :], y.loc[y['fold'] == i, :]\n",
    "    sev_mask = y_train['claim_cost'] > 0.0\n",
    "    sev_stack.fit(X_train.loc[sev_mask, :], y_train.loc[sev_mask, 'severity'], transformedtargetregressor__sample_weight=y_train.loc[sev_mask, 'claim_count'])\n",
    "    preds = sev_stack.predict(X_test)\n",
    "    train_preds.append(pd.Series(preds, index=X_test.index))\n",
    "    sevs.append(np.sqrt(np.mean((y_test.loc[y_test['claim_cost'] > 0.0, 'severity'] - preds[y_test['claim_cost'] > 0.0]) ** 2)))\n",
    "sev_stack_preds = pd.concat(train_preds).loc[y.index]\n",
    "print(sevs)\n",
    "print(np.mean(sevs))\n",
    "sev_mask = y['claim_cost'] > 0.0\n",
    "sev_stack.fit(X_cv.loc[sev_mask, :], y.loc[sev_mask, 'severity'], transformedtargetregressor__sample_weight=y.loc[sev_mask, 'claim_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25098010417921907"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gini(y['claim_cost'], sev_stack_preds * freq_stack_preds * y['exposure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1367.68410516, 2078.82416217, 1157.41232085, ..., 3327.27750518,\n",
       "       3844.92316855, 1544.40888525])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sev_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our submission\n",
    "X_cv_freq = pd.concat([df_test[lin_cols], pd.DataFrame(test_freq_preds)], axis=1)\n",
    "X_cv_sev = pd.concat([df_test[lin_sev_cols], pd.DataFrame(test_sev_preds)], axis=1)\n",
    "freq_preds = freq_stack.predict(X_cv_freq)\n",
    "sev_preds = sev_stack.predict(X_cv_sev)\n",
    "df_test['claim_cost'] = df_test['exposure'] * freq_preds * sev_preds\n",
    "df_test['id'] = np.arange(df_test.shape[0])\n",
    "df_test['id'] = df_test['id'].astype(int)\n",
    "df_test['id'] += 1\n",
    "df_test[['id', 'claim_cost']].to_csv('stacked_freq_sev_predictions_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing just using logistic regression with exposure as a feature\n",
    "from catboost import CatBoostClassifier\n",
    "ind_cols = ['veh_value', 'veh_body', 'veh_age', 'gender', 'area', 'dr_age', 'exposure']\n",
    "ind_cat = CatBoostClassifier(n_estimators=500,\n",
    "                             max_depth=4,\n",
    "                             subsample=0.67,\n",
    "                             cat_features=['veh_body', 'gender', 'area'],\n",
    "                             #monotone_constraints=[csts[i] if i in csts.keys() else 0 for i in boost_cols],\n",
    "                             auto_class_weights='Balanced',\n",
    "                             verbose=0,\n",
    "                             thread_count=-1)\n",
    "\n",
    "# Getting predictions\n",
    "probs = cross_val_predict(ind_cat,\n",
    "                          X[ind_cols],\n",
    "                          y['claim_ind'],\n",
    "                          groups=y['fold'],\n",
    "                          cv=GroupKFold(n_folds),\n",
    "                          method='predict_proba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.63      0.76     21076\n",
      "           1       0.10      0.58      0.17      1534\n",
      "\n",
      "    accuracy                           0.63     22610\n",
      "   macro avg       0.53      0.61      0.47     22610\n",
      "weighted avg       0.90      0.63      0.72     22610\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y['claim_ind'], probs[:,1] > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x2ace9f56b48>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_cat.fit(X[ind_cols], y['claim_ind'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     feature       gain\n",
      "6   exposure  40.110706\n",
      "0  veh_value  25.975083\n",
      "4       area  10.205096\n",
      "1   veh_body   9.757386\n",
      "5     dr_age   9.138234\n",
      "2    veh_age   3.022741\n",
      "3     gender   1.790754\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame({'feature': ind_cols, 'gain': ind_cat.feature_importances_}).sort_values('gain', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('target_enc',\n",
       "                                                  TargetEncoder(min_samples=20),\n",
       "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x000002ACE9F75FC8>)])),\n",
       "                ('standardscaler', StandardScaler()),\n",
       "                ('transformedtargetregressor',\n",
       "                 TransformedTargetRegressor(regressor=LinearSVR(loss='squared_epsilon_insensitive',\n",
       "                                                                max_iter=3000),\n",
       "                                            transformer=StandardScaler()))])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick severity model\n",
    "lsvr_sev = make_pipeline(ColumnTransformer([('target_enc', TargetEncoder(True, True, 20), get_cats)\n",
    "                                      ], remainder='passthrough'),\n",
    "                    StandardScaler(),\n",
    "                    TransformedTargetRegressor(LinearSVR(loss='squared_epsilon_insensitive', C=1.0, max_iter=3000), transformer=StandardScaler()))\n",
    "sev_mask = y['claim_cost'] > 0.0\n",
    "lsvr_sev.fit(X.loc[sev_mask, lin_sev_cols], y.loc[sev_mask, 'severity'], transformedtargetregressor__sample_weight=y.loc[sev_mask, 'claim_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x2acea6db108>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Freq sev predictions\n",
    "csts = {'veh_value': 1, 'dr_age': -1}\n",
    "sev_cat = CatBoostRegressor(n_estimators=250,\n",
    "                            subsample=0.67,\n",
    "                            max_depth=1,\n",
    "                            cat_features=[i for i in boost_sev_cols if i not in ['veh_value', 'dr_age', 'veh_age', 'exposure']],\n",
    "                            monotone_constraints=[csts[i] if i in csts.keys() else 0 for i in boost_sev_cols],\n",
    "                            verbose=0,\n",
    "                            thread_count=-1)\n",
    "\n",
    "sev_mask = y['claim_cost'] > 0.0\n",
    "sev_cat.fit(X.loc[sev_mask, boost_sev_cols], y.loc[sev_mask, 'severity'], sample_weight=y.loc[sev_mask, 'claim_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding exponent to boost inequality\n",
    "preds = (ind_cat.predict_proba(df_test[ind_cols])[:,1] ** 5) * lsvr_sev.predict(df_test[lin_sev_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  8.79745768, 114.74007332, 105.43851636, ..., 145.23840301,\n",
       "        17.13076749,   0.15929425])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test our submission by ID, something doesn't seem right there\n",
    "# Creating our submission\n",
    "submission = df_test[['id']].copy()\n",
    "submission['id'] = np.arange(submission.shape[0])\n",
    "submission['id'] = submission['id'].astype(int)\n",
    "submission['id'] += 1\n",
    "submission['claim_cost'] = preds\n",
    "submission[['id', 'claim_cost']].to_csv('yet_another_submission_3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
